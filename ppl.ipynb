
import nltk

corpus = "Hello, I am Ayush Sharma."
"Welcome to GenAI NLP workspace."

nltk.download('punkt_tab')

from nltk.tokenize import word_tokenize

word_tokenize(corpus)


msg = "Hi, we are learning nlp genAI today at Oracle"

from nltk.tokenize import wordpunct_tokenize

wordpunct_tokenize(msg)

from nltk.stem import PorterStemmer

obj_stemming = PorterStemmer()

obj_stemming.stem("running")

obj_stemming.stem("history")

nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer

obj = WordNetLemmatizer()

obj.lemmatize("history")

# lemmatization is better than stemming
obj.lemmatize("running")

obj.lemmatize("going", pos='v')

from nltk.corpus import stopwords
nltk.download('stopwords')

stopwords.words('english')

corpus = "Artificial intelligence is transforming the way we interact with technology. " \
"From natural language processing to computer vision, AI systems are becoming increasingly sophisticated. " \
"These advancements enable machines to understand, learn, and respond to human input. As research continues, the potential applications of AI in healthcare, education, and business are expanding rapidly, shaping our future."

from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
stemmer = PorterStemmer()
docs = nltk.sent_tokenize(corpus)
print(type(docs), len(docs))

for var in range(len(docs)):
    words = nltk.word_tokenize(docs[var])
    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]
    docs [var] = ' '.join(words)
print(docs)

from nltk.util import ngrams
from nltk.tokenize import word_tokenize

data = "I love to learn nlp and genAI with Oracle"
token = word_tokenize(data)
list(ngrams(token, 2))
